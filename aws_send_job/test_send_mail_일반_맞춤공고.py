import boto3
from botocore.exceptions import ClientError
import mysql.connector
import json
import os
import logging
import re
import mariadb
import asyncio
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dotenv import load_dotenv
from sqlalchemy import create_engine

import time
import logging
import copy
import pandas as pd
import random
import gspread
import urllib.parse

from _call_ import * 

load_dotenv()

# selenium ì„¤ì •
options = Options()
options.add_argument('--headless')
options.add_argument('--disable-gpu')
options.add_argument('--no-sandbox')
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36")


# html ë¶ˆëŸ¬ì˜¤ê¸°
html_header_tr = read_html_file('./HTML_CODE/header.html')
html_site_tr = read_html_file('./HTML_CODE/site_tr.html')
html_job_tr = read_html_file('./HTML_CODE/job_tr.html')

# api_gateway 
api_gateway_url = "https://n5oeg83wll.execute-api.ap-northeast-2.amazonaws.com/active/jobclick"

site_name_kr = {'saramin': 'ì‚¬ëŒì¸','wanted': 'ì›í‹°ë“œ','jumpit': 'ì í•','incruit': 'ì¸í¬ë£¨íŠ¸'}


# í¬í•¨, ì œì™¸ í‚¤ì›Œë“œ ì •í•˜ê¸°
def create_job_conditions(course):
    course_upper = course.upper()
    if "BIGDATA" in course_upper:
        return JOB_KEYWORDS["BIGDATA"], STOPWORDS["BIGDATA"]
    elif "FULLSTACK" in course_upper:
        return JOB_KEYWORDS["FULLSTACK"], STOPWORDS["FULLSTACK"]
    elif "PM" in course_upper:
        return JOB_KEYWORDS["PM"], STOPWORDS["PM"]
    else:
        return []




# course : ê³¼ì •ëª…(ex.BIGDATA) / job_sites : ['saramin', 'wanted', 'jumpit', 'incruit']
def keyword_query(course, job_sites):
    db_connection = connect_to_job()
    query_file = "./query_base.sql"
    
    # í¬í•¨í‚¤ì›Œë“œ, ì œì™¸í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    include, exclude = create_job_conditions(course)

    # ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë§Œë“¤ê¸° (sql ì¿¼ë¦¬ë¬¸ì— ë“¤ì–´ê°ˆ í¬í•¨í‚¤ì›Œë“œ, ì œì™¸í‚¤ì›Œë“œ ì¿¼ë¦¬ë¬¸ìœ¼ë¡œ ë§Œë“¤ê¸°ìœ„í•´ì„œ)
    include_dict = {} # ê³µê³ ëª…(job_title) ì ìš©
    exclude_dict1 = {} # ê³µê³ ëª…(job_title) ì ìš©
    exclude_dict2 = {} # ê¸°ì—…ëª…(company_name) ì ìš©

    # ì±„ìš©ê³µê³  ì‚¬ì´íŠ¸ ë³„ë¡œ ì¿¼ë¦¬ë¬¸ ì‘ì„±
    for site in job_sites:
        include_dict[f"include_keyword_{site}"] = " OR ".join([f"{site}_data.job_title LIKE '%{word}%'" for word in include])
        exclude_dict1[f"exclude_keyword_{site}"] = " AND ".join([f"{site}_data.job_title NOT LIKE '%{word}%'" for word in exclude])
        exclude_dict2[f"exclude_keyword_{site}"] = " AND ".join([f"{site}_data.company_name NOT LIKE '%{word}%'" for word in exclude])

    # ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ ì¿¼ë¦¬ë¬¸ ë§Œë“¤ê¸°~
    query_dict = {
            site: f"({include_dict[f'include_keyword_{site}']}) AND ({exclude_dict1[f'exclude_keyword_{site}']}) AND ({exclude_dict2[f'exclude_keyword_{site}']})"
            for site in job_sites
        }

    with open(query_file, "r", encoding="utf-8") as file:
        query_content = file.read()

    for site in job_sites:
        query_content = query_content.replace(f"{{{site}}}", query_dict[site])
    
    cursor = db_connection.cursor()
    cursor.execute(query_content)
    columns = [column[0] for column in cursor.description]
    df = pd.DataFrame(cursor.fetchall(), columns=columns)
    df['course'] = course

    print(f'[{course}--------------ì´ ê³µê³  ìˆ˜{len(df)}]')
    
    ##################### ì´ì „ì— ë³´ë‚¸ ê³µê³  ì‚­ì œ ######################
    select_sql = """SELECT recruit_url from send_job;"""

    cursor.execute(select_sql)
    urls = cursor.fetchall()
    urls_list = [row[0] for row in urls]
    cursor.close()
    db_connection.close()
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•¨.
    # df = df.groupby('source_table').head(5).reset_index(drop=True)
    df = df[~df['recruit_url'].isin(urls_list)]
    ################################################################


    #################### ìµœì‹ ìˆœìœ¼ë¡œ ì „ì²˜ë¦¬ #########################
    df_date = df[df['deadline'].str.contains(r'^\d{4}-\d{2}-\d{2}$', na=False)].copy()
    df_nondate = df[~df['deadline'].str.contains(r'^\d{4}-\d{2}-\d{2}$', na=False)].copy()

    df_date['deadline'] = pd.to_datetime(df_date['deadline']).dt.strftime('%Y-%m-%d')

    df_date_sorted = df_date.sort_values(by='deadline', ascending=False)

    df = pd.concat([df_date_sorted, df_nondate], ignore_index=True)
    ###############################################################
    return  df


# ë§ˆê°ê³µê³  ë° ì´ì „ ë³´ë‚¸ ê³µê³  íŒŒì•…
def job_check(df):
    stop_words = ['ë§ˆê°ëœ ê³µê³ ', 'ë§ˆê°ëœ í¬ì§€ì…˜', 'ì§€ì›ë§ˆê°', 'ì ‘ìˆ˜ë§ˆê°']
    bad = []
    good = []
    seen = set()
    driver = webdriver.Chrome(options)
    driver.set_window_size(1000, 1350)

    today = datetime.today()

    for idx, row in df.iterrows():
        time.sleep(random.uniform(1, 4))
        url = row['recruit_url']
        source = row['source_table']
        deadline_str = str(row['deadline']).strip()

        # ì œê³µ í•  ê³µê³ ì—ì„œ ê³¼ì • ë³„ ì¹´ìš´íŠ¸
        source_counts = {
            'saramin': 0,
            'wanted': 0,
            'jumpit': 0,
            'incruit': 0
        }
        for g in good:
            s = g['source_table']
            if s in source_counts:
                source_counts[s] += 1

        # ë§Œì•½ í˜„ì¬ ì‚¬ì´íŠ¸ì˜ ê³µê³ ê°€ ì´ë¯¸ 20ê°œë©´ ê±´ë„ˆëœ€
        if source_counts.get(source, 0) >= 20:
            print(f"[PASS]-- [{source}] 20ê°œ ì´ˆê³¼, ê±´ë„ˆëœ€: {row['company_name']} - {row['job_title']}")
            continue

        # ë‚ ì§œ í˜•ì‹ í™•ì¸ í›„ 7ì¼ ì´í•˜ ë‚¨ì€ ê³µê³ ëŠ” ê±´ë„ˆëœ€
        try:
            deadline_date = datetime.strptime(deadline_str, '%Y-%m-%d')
            days_left = (deadline_date - today + timedelta(days=1)).days
            if days_left < 1:
                print(f"[SKIP] {row['company_name']} - {row['job_title']} ({days_left}ì¼ ë‚¨ìŒ)")
                continue
        except:
            deadline_date = None

        # ê³µê³  í™•ì¸
        driver.get(url)
        try:
            if 'jumpit' in url:
                text = driver.find_element(By.CLASS_NAME, 'sc-190eb830-1.gaBEGj').text
            elif 'wanted' in url:
                text = driver.find_element(By.CLASS_NAME, 'WantedApplyBtn_container__lBx_L').text
            elif 'incruit' in url:
                text = driver.find_element(By.CLASS_NAME, 'btn_bgrp.btn_jobapp.off').text
            elif 'saramin' in url:
                text = driver.find_element(By.CLASS_NAME, 'sri_btn_expired_apply').text
            else:
                text = ''
        except:
            print(f'class_nameì„ ì°¨ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. :{url}')
            text = 'ì—†ì–´!'

        # íšŒì‚¬+ê³µê³ ëª… ì¤‘ë³µ ì œê±°ìš© í‚¤
        combined = re.sub(r'\(?ì£¼ì‹íšŒì‚¬\)?|\(?ì£¼\)?|\(?ìœ \)?', '', row['company_name']).strip() + ' ' + row['job_title'].strip()

        if text in stop_words:
            bad.append(row)
        elif combined not in seen:
            good.append(row)
            seen.add(combined)

        print(f"[GOOD] {source_counts}")

        # ê° ì‚¬ì´íŠ¸ 3ê°œ ì´ìƒì´ë©´ ë 
        if all(count >= 7 for count in source_counts.values()):
            print("[âœ…] ì‚¬ì´íŠ¸ë³„ë¡œ 7ê°œì”© ìˆ˜ì§‘ ì™„ë£Œ. ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            break

    driver.quit()
    bad_df = pd.DataFrame(bad)
    good_df = pd.DataFrame(good)

    return bad_df, good_df

# ses ë©”ì¼ ë°œì†¡ ì½”ë“œ
def ses_mail(sender_email, recipient_email, subject, html_body):
    aws_region = os.getenv('aws_region')
    aws_access_key_id = os.getenv('aws_access_key_id')
    aws_secret_access_key = os.getenv('aws_secret_access_key')
    
    # ses_client ìƒì„±
    ses_client = boto3.client('ses',
                              region_name = aws_region,
                              aws_access_key_id = aws_access_key_id,
                              aws_secret_access_key = aws_secret_access_key
                              )
    
    # 'chunjae'ë¼ëŠ” ë¬¸ìì—´ì´ recipient_emailì— í¬í•¨ëœ ê²½ìš° ConfigurationSetNameì„ ìƒëµ
    config_set = None  # ê¸°ë³¸ì ìœ¼ë¡œ Configuration Setì„ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ì„¤ì •

    if 'chunjae' in recipient_email:  # ì œëª©ì— 'chunjae'ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´
        config_set = None  # ConfigurationSetNameì„ Noneìœ¼ë¡œ ì„¤ì •
    else:
        config_set = 'observe'  # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 'observe'ë¼ëŠ” Configuration Setì„ ì‚¬ìš©
    
    try:
        response = ses_client.send_email(
            Destination = {
                'ToAddresses': [recipient_email],
            },
            Message = {
                'Body' : {
                    'Html' : {
                        'Charset' : 'UTF-8',
                        'Data' : html_body
                    },
                },
                'Subject' : {
                    'Charset' : 'UTF-8',
                    'Data' : subject
                },
            },
            **({"ConfigurationSetName": config_set} if config_set else {}),
            Source = f'"ì²œì¬ITêµìœ¡ì„¼í„°" <{sender_email}>',
        )
        logging.info(f"Email sent successfully to {recipient_email}")
    except ClientError as e:
        logging.error(f"Error sending email: {e.response['Error']['Message']}")
    else:
        logging.info("Email sent successfully")



# ë©”ì¼ ë³´ë‚´ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš© / êµìœ¡ìƒì—ê²Œ ë³´ëƒ„
def send_mail(name, email, course, very_good_df, job_sites):
    nomal_tr = "" 
    job_tr = ""
    total_job_postings = 0

    # ë³´ë‚´ëŠ” ê³µê³  ëª¨ìŒì§‘(dbì— ì ì¬í•˜ë ¤ê³ )
    sent_df = pd.DataFrame(columns=very_good_df.columns)
    for site in job_sites:
        # kr_name = site_name_kr.get(site)
        
        # ì‚¬ëŒì¸ìœ¼ë¡œ ëª»ì±„ìš´ ê°¯ìˆ˜ ì±„ìš°ë ¤ê³  ì‚¬ëŒì¸ì€ ë‹¤ ê°€ì ¸ì˜´(ì´ 20ê°œ)
        if site =='saramin':
            df_ = very_good_df[very_good_df['source_table'] == site].head(20-total_job_postings)
        else:
            df_ = very_good_df[very_good_df['source_table'] == site].head(5)

        # # ì±„ìš©ê³µê³ ê°€ ì—†ëŠ” ì‚¬ì´íŠ¸ ì œì™¸
        # if df_.empty:
        #     continue

        for idx, row in df_.iterrows():
            company = row['company_name']
            deadline = row['deadline']
            title = row['job_title']
            job_url = row['recruit_url']
            open_source = 'mail'
            # ê³µê³  ì •ë¦¬
            encoded_url = urllib.parse.quote(job_url, safe='/')
            job_url = f"{api_gateway_url}?user_id={email}&name={name}&url={encoded_url}&course={course}&open_source={open_source}"
            # ê° ì±„ìš©ê³µê³  ë³„ html ëª¨ìœ¼ê¸°
            job_tr += html_job_tr.format(job_url,company,deadline,title)
            total_job_postings += 1

        # ë³´ë‚´ëŠ” ê³µê³  ëˆ„ì 
        sent_df = pd.concat([sent_df, df_])

        # ì±„ìš©ê³µê³  ì‚¬ì´íŠ¸ ë³„ table html
    nomal_tr += html_site_tr.format('ğŸ” ì±„ìš© ì¤‘ì¸ ê³¼ì • ì¶”ì²œ ê³µê³ ', job_tr)
    
    today = datetime.today().strftime("%Y-%m-%d")
    
    # ìµœì¢… html
    html_res = html_header_tr.format(today, name, total_job_postings, nomal_tr)

    sender_email = 'chunjaecloud@gmail.com'
    
    recipient_email = email

    subject = f'ğŸ“Œ [ì²œì¬ITêµìœ¡ì„¼í„°] {name}ë‹˜! ì´ë²ˆì£¼ {course} ì±„ìš©ê³µê³ ëŠ”?'
    feedback_ = 'ğŸ“Œ [ì²œì¬ITêµìœ¡ì„¼í„°] ì±„ìš©ê³µê³  í”¼ë“œë°± ìš”ì²­' 

    ses_mail(sender_email, recipient_email, subject, html_res) 
    
    # ë³´ë‚¸ ê³µê³  dbì— ì ì¬
    db_user = os.getenv("Job_db_user")
    db_password = os.getenv("Job_db_password")
    db_host = os.getenv("Job_db_host")
    db_port = int(os.getenv("Job_db_port", 3306))
    db_name = "crawling"

    engine = create_engine(
        f"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    )

    # ì´ë¯¸ DBì— ìˆëŠ” recruit_url ë¶ˆëŸ¬ì˜¤ê¸°
    existing_urls = pd.read_sql("SELECT recruit_url FROM send_job", con=engine)
    existing_urls_set = set(existing_urls['recruit_url'])

    # ì¤‘ë³µ ì œê±°
    sent_df = sent_df[~sent_df['recruit_url'].isin(existing_urls_set)]

    # ê³µê³ ê°€ ìˆì„ ë•Œë§Œ ì ì¬
    if not sent_df.empty:
        sent_df.to_sql(name="send_job", con=engine, if_exists="append", index=False)

def save_db(final_df):
    db_user = os.getenv("Job_db_user")
    db_password = os.getenv("Job_db_password")
    db_host = os.getenv("Job_db_host")
    db_port = int(os.getenv("Job_db_port", 3306))
    db_name = "crawling"

    engine = create_engine(
        f"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    )

    print('save start')

    final_df.to_sql(name="send_job", con=engine, if_exists='append', index=False)
    
    print('save end')


# ë©”ì¸ ì½”ë“œ 
def main(df_students):
    # ìµœì¢… ê³µê³  
    good_job_list = []
    
    courses = ['BIGDATA', 'FULLSTACK', 'PM']
    # ë§ˆì§€ë§‰ì— ì‚¬ëŒì¸ìœ¼ë¡œ ì• 3ê°œ ì‚¬ì´íŠ¸ì˜ ë¶€ì¡±í•œ ê³µê³ ë¥¼ ë§¤ê¿ˆ ê·¸ë˜ì„œ ê¼­ ì‚¬ëŒì¸ì´ ë§ˆì§€ë§‰ì´ì—¬ì•¼ í•¨.
    job_sites = ['wanted', 'jumpit', 'incruit','saramin']
    for course in courses:
        # ì¿¼ë¦¬ë¬¸ìœ¼ë¡œ ì±„ìš©ê³µê³  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        good = keyword_query(course,job_sites)
        _, good_df = job_check(good)
        good_job_list.append(good_df)
    # ëª¨ë“  ê³¼ì • final ì±„ìš©ê³µê³ 
    final_df = pd.concat(good_job_list, ignore_index=True)
    # ë§ˆê°ê³µê³  ë° ì´ì „ì— ë³´ë‚¸ ê³µê³  íŒŒì•…   

    save_db(final_df)
    
    # for idx,row in df_students.iterrows():
    #     name =row['name']
    #     email = row['email']
    #     course = row['subject']
    #     very_good_df = final_df[final_df['course'] == course]
    #     send_mail(name, email, course, very_good_df, job_sites)

# send_mail í•¨ìˆ˜ë‘ ê³µê³  ë³´ë‚´ëŠ” main ì½”ë“œ ì‘ì„±í•´ì•¼ëŒ.


######################################################################################
######################################################################################
######################################################################################
###################################################################################### 


# ìˆ˜ì‹  ê±°ë¶€ ì¸ì› send_email = 2 ë°”ê¾¸ê¸°

# ë°” ("UnsubListì˜ ì‚¬ë³¸")
json_file_path = "/home/ubuntu/job_posting/GOOGLE_API/genia_email-recommand-6976a7d469c3.json" 
gc = gspread.service_account(json_file_path) 
spreadsheet_url = "https://docs.google.com/spreadsheets/d/1GdC3sv6q-t2v25alAmS83M76eDsrhfZBwTFOrd0Q1jw/edit?resourcekey=&gid=277815760#gid=277815760"
worksheet = gc.open_by_url(spreadsheet_url)
sheet = worksheet.worksheet("UnsubListì˜ ì‚¬ë³¸")
rows = sheet.get_all_values()

update_sql = """UPDATE job_member SET send_email = 2 WHERE email = %s"""
conn = connect_to_lms_test()
cursor = conn.cursor()

email_list = []
for idx, row in enumerate(rows[1:], start=2):  # ì²« ë²ˆì§¸ í–‰(í—¤ë”) ì œì™¸
    if len(row) > 2 and '/' in row[2]:
        name, email = row[2].split('/')[0].strip(), row[2].split('/')[1].strip()
        email_list.append(email)

for email in email_list:
    cursor.execute(update_sql, (email,))

conn.commit()
cursor.close()
conn.close()

#######################################

# lms ì—ì„œ ì±„ìš©ê³µê³  ë³´ë‚¼ ì¸ì› íŒŒì•… 
conn = connect_to_lms_test()
cursor = conn.cursor()

#* lms_test ë°”ê¿”ì•¼ í•¨.
update_sql = """
UPDATE lms_test.job_member
LEFT JOIN lms.course ON lms_test.job_member.cno = lms.course.no
SET lms_test.job_member.send_email = 1
WHERE lms_test.job_member.send_email IN (0, 3)
  AND lms_test.job_member.status NOT IN ('REST', 'OUT','EMPLOYED')
  AND DATEDIFF(CURDATE(), lms.course.start_date) >= 140; 
"""
cursor.execute(update_sql)
conn.commit()

#* lms_test ë°”ê¿”ì•¼ í•¨.
# 2. send_emailì´ 1ì¸ ì‚¬ëŒë“¤ë§Œ ì¡°íšŒ
select_sql = """
SELECT  
    lms_test.job_member.*, 
    lms.course.start_date, 
    lms.course.end_date,
    lms.course.subject, 
    lms.course.flag   
FROM lms_test.job_member
LEFT JOIN lms.course ON lms_test.job_member.cno = lms.course.no
WHERE lms_test.job_member.send_email = 1
AND lms_test.job_member.role = 'student';
"""

cursor.execute(select_sql)
columns = [column[0] for column in cursor.description]
df_students = pd.DataFrame(cursor.fetchall(), columns=columns)
print(df_students)
cursor.close()
conn.close()



# ê³¼ì • ë³„ í‚¤ì›Œë“œ ë° ë¶ˆìš©ì–´
JOB_KEYWORDS = {
    "BIGDATA": [
        'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸', 'ë°ì´í„° ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ë¶„ì„ê°€', 'ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸', 'ë°ì´í„°ì—”ì§€ë‹ˆì–´', 'ë°ì´í„°ë¶„ì„ê°€',
        'ë°ì´í„° ë¶„ì„','ë°ì´í„°ë¶„ì„', 'ë°ì´í„° ì •ì œ', 'ë°ì´í„°ì •ì œ', 'ë°ì´í„° ì²˜ë¦¬', 'ë°ì´í„°ì²˜ë¦¬', 'ai ê¸°íš', 'AI ê¸°íš', 'aiê¸°íš',
        'ë°ì´í„° ê´€ë¦¬', 'ë°ì´í„° ë¶„ì„ ë§¤ë‹ˆì €', 'ë¨¸ì‹ ëŸ¬ë‹', 'AI ì—”ì§€ë‹ˆì–´', 'ì¸ê³µì§€ëŠ¥ ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ì‹œê°í™”', 'tableau', 'Tableau'
        'ë°ì´í„° ë§ˆì´ë‹', 'ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤', 'ETL ê°œë°œ', 'SQL', 'R ë¶„ì„', 'Hadoop', 'ì—ë“€í…Œí¬ ì½˜í…ì¸  ê°œë°œ', 'ë”¥ëŸ¬ë‹/ë¨¸ì‹ ëŸ¬ë‹',
        'Data Scientist', 'Data Engineer', 'Data Analyst', 'Machine Learning', 'AI Engineer', 'Data Visualization', 
        'Business Intelligence', 'ETL Developer', 'Big Data Engineer', 'Data Management', 'Data Mining', 'ë”¥ëŸ¬ë‹', 'ìì—°ì–´ ì²˜ë¦¬',
        'Deep Learning Engineer', 'Natural Language Processing', 'NLP', 'DBA', 'ë¹…ë°ì´í„°' 'AI ëª¨ë¸','DBê´€ë¦¬', 'dbê´€ë¦¬',
        'ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸', 'ì¸ê³µì§€ëŠ¥', 'ë°ì´í„°ë¶„ì„','ë°ì´í„°ì²˜ë¦¬','ë°ì´í„°ê´€ë¦¬','ë°ì´í„°ë§ˆì´ë‹', 'LLM','Data Architect','ë°ì´í„° ë¦¬í„°ëŸ¬ì‹œ'],
    "FULLSTACK": [
        'í”„ë¡ íŠ¸ì—”ë“œ', 'í”„ë¡ íŠ¸ì•¤ë“œ' 'ë°±ì—”ë“œ', 'ë°±ì•¤ë“œ', 'ì›¹ ì„œë¹„ìŠ¤', 'ëª¨ë°”ì¼ ì„œë¹„ìŠ¤','í’€ìŠ¤íƒ', 'Java', 'ì›¹ ê°œë°œì',
        'ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´', 'ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì', 'ì‹œìŠ¤í…œ ì—”ì§€ë‹ˆì–´', 'ì‹œìŠ¤í…œ ê°œë°œì', 'ëª¨ë°”ì¼ ê°œë°œì',
        'ì•± ê°œë°œì', 'API ê°œë°œì', 'í´ë¼ìš°ë“œ ì—”ì§€ë‹ˆì–´', 'DevOps ì—”ì§€ë‹ˆì–´', 'DevOps ê°œë°œì', 'ì„œë²„ ê°œë°œì',
        'ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´', 'ë„¤íŠ¸ì›Œí¬ ê°œë°œì', 'Front-End Developer', 'Front-End', 'Back-End'
        'Back-End Developer', 'Full Stack Developer', 'Software Engineer', 'Mobile Developer', 'Cloud Engineer', 
        'API Developer', 'DevOps Engineer', 'Server Developer', 'Web Designer', 'ì›¹ í¼ë¸”ë¦¬ì…”','Vue.js', 
        'Node.js', 'Frontend Engineer', 'Backend Engineer', 'React Developer', 'Node.js Developer', 'Vue.js Developer', 'CI/CD'
    ],
    "PM": [
        'ì„œë¹„ìŠ¤ ê¸°íš', 'ì„œë¹„ìŠ¤ê¸°íš', 'í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €', 'í”„ë¡œë•íŠ¸ë§¤ë‹ˆì €', 'ì„œë¹„ìŠ¤ ê¸°íšì','ì„œë¹„ìŠ¤ê¸°íšì','ì„œë¹„ìŠ¤ ê¸°íš ë§¤ë‹ˆì €', 'ì„œë¹„ìŠ¤ ê¸°íš PM', 
        'ì„œë¹„ìŠ¤ ê¸°íš PL', 'ì„œë¹„ìŠ¤ ê¸°íš ë‹´ë‹¹ì', 'ì½˜í…ì¸  ê¸°íš','ì½˜í…ì¸ ê¸°íš','ì½˜í…ì¸ ê¸°íšì','ì½˜í…ì¸  ê¸°íšì' 'êµìœ¡ ì½˜í…ì¸  ê¸°íš','product manage',
        'product manager', 'PRODUCT MANAGE', 'PRODUCT MANAGER','ì„œë¹„ìŠ¤ ìš´ì˜ ë§¤ë‹ˆì €','ì„œë¹„ìŠ¤ ìš´ì˜ ê¸°íš', 'ë””ì§€í„¸ ê¸°íšì', 'ì‚¬ì—… ê¸°íš', 'í”Œë«í¼ ë§¤ë‹ˆì €',
        'í”„ë¡œë•íŠ¸ ì˜¤ë„ˆ', 'í”„ë¡œì íŠ¸ ê¸°íš', 'UX ê¸°íšì', 'UI ê¸°íšì', 'PRODUCT OWNER' , 'Product Owner', 'product owner'
    ]
}

STOPWORDS = {
    "BIGDATA": [
        'ì„ì‚¬', 'ë°•ì‚¬', 'ì½”ë“œì‡', 'êµìœ¡ìƒ', 'êµ­ë¹„', 'ì²­ë…„ìˆ˜ë‹¹', 'ì œì•½', 'ê±´ì„¤', 'ì—°êµ¬', 'ê¸°ê³„', 'í–‰ì •', 'í˜¸í…”', 'ì²œì¬êµìœ¡', 
    'ì²œì¬êµê³¼ì„œ', 'SeSAC', 'APM', 'apm', 'edi', 'EDI', 'ë§ˆì¼€í„°', 'ì–‘ì„±', 'ê¸°íš', 'ì„¼í„°' ,'ê²½ë ¥ì§', 'ê²½ë ¥ì'
    ],
    "FULLSTACK": [
        'ì„ì‚¬', 'ë°•ì‚¬', 'ì½”ë“œì‡', 'êµìœ¡ìƒ', 'êµ­ë¹„', 'ì²­ë…„ìˆ˜ë‹¹', 'ì œì•½', 'ê±´ì„¤', 'ì—°êµ¬', 'ê¸°ê³„', 'í–‰ì •', 'í˜¸í…”', 'ì²œì¬êµìœ¡', 
    'ì²œì¬êµê³¼ì„œ', 'SeSAC', 'APM', 'apm', 'edi', 'EDI', 'UX', 'UI', 'ë§ˆì¼€í„°','ì–‘ì„±','ê¸°íš', 'ê²½ë ¥ì§', 'ê²½ë ¥ì'
    ],
    "PM": [
        'ì°½ì—…', 'ë²ˆì—­', 'ì„ì‚¬', 'ë°•ì‚¬', 'ì½”ë“œì‡', 'êµìœ¡ìƒ', 'êµ­ë¹„', 'ì²­ë…„ìˆ˜ë‹¹', 'ì œì•½', 'ê±´ì„¤', 'ì—°êµ¬', 'ê¸°ê³„', 'í–‰ì •', 'í˜¸í…”',
         'ì²œì¬êµìœ¡', 'ì²œì¬êµê³¼ì„œ', 'SeSAC', 'APM', 'apm', 'edi', 'EDI', 'ê°œë°œì', 'ì‹œê³µ','ìœ íŠœë¸Œ','ì–‘ì„±', 'ê²½ë ¥ì§', 'ê²½ë ¥ì',
         'ë¬¼ë¥˜', 'ì „ê¸°ì°¨' ,
    ]
}


main(df_students)